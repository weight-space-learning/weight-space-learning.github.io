---

layout: default
---


The recent surge in the number of publicly available neural network models—exceeding a million on platforms like Hugging Face—calls for a shift in how we perceive neural network weights. This workshop aims to establish neural network weights as a new data modality, offering immense potential across various fields.

We plan to address key dimensions of weight space learning:

- **Model analysis**: Inferring model properties, such as generalization error, by inspecting weights.
- **Model synthesis**: Generating new models and improving tasks like pruning, merging, and robustness through weight manipulation.
- **Learning from populations**: Using data from neural network populations to automate tasks like hyperparameter selection or optimizing architectures.
- **Applications to neural fields**: Applying weight space learning to 3D shape analysis, neural radiance fields (NeRFs), and other domains.

Weight space learning remains a nascent and scattered research area. Our goal is to unite researchers from subfields like model merging, neural architecture search, and meta-learning, and create a common language for this emerging field. By aligning terminology and methodologies, we aim to drive sustained progress and foster interdisciplinary collaboration.

## Research Goals and Key Questions

This workshop will explore fundamental questions about weight spaces, such as:

- What are the inherent properties of weights, like symmetries and invariances, that affect learning and optimization?
- How can model weights be efficiently represented, processed, and used for downstream tasks?
- What information can be decoded from model weights?
- Can we generate model weights for specific applications, such as transfer learning or neural fields?
- How can we democratize the usage of weight spaces for more efficient research and applications?

## Call for Papers

We invite submissions in the form of extended abstracts (4-6 pages) or full papers (8-12 pages) on topics including but not limited to:

- **Weight Space as a Modality**: Characterization of symmetries, scaling laws, and model zoo datasets.
- **Learning Tasks/Paradigms**: Weight embeddings, meta-learning, autoencoders, and weight space backbones like MLPs and transformers.
- **Theoretical Foundations**: Expressivity of weight space modules and generalization bounds.
- **Model/Weight Analysis**: Inferring model properties and exploring neural lineage.
- **Weight Synthesis**: Weight sampling, model merging, and generating weights for specific tasks.
- **Applications**: Computer vision (e.g., NeRFs/INRs), physics modeling, and adversarial robustness.

Accepted contributions will be presented in poster sessions and spotlight talks.

## Tentative Schedule

| Time      | Duration | Session Content                                   |
| --------- | -------- | ------------------------------------------------- |
| 9:00 AM   | 30 min   | Introduction and opening remarks                  |
| 9:30 AM   | 60 min   | Opening Keynote                                   |
| 10:30 AM  | 30 min   | Coffee Break                                      |
| 11:00 AM  | 15 min   | Session 1: Graphs and Symmetries                  |
| 11:15 AM  | 30 min   | Invited Talk + Q&A                                |
| 11:45 AM  | 30 min   | 2x Spotlight Talks (each 15 min)                  |
| 12:15 PM  | 75 min   | Lunch Break                                       |
| 1:30 PM   | 15 min   | Session 2: Representation Learning                |
| 1:45 PM   | 30 min   | Invited Talk + Q&A                                |
| 2:15 PM   | 30 min   | 2x Spotlight Talks (each 15 min)                  |
| 2:45 PM   | 75 min   | Coffee Break + Poster Session                     |
| 4:00 PM   | 15 min   | Session 3: Downstream Applications                |
| 4:15 PM   | 30 min   | Invited Talk + Q&A                                |
| 4:45 PM   | 15 min   | Closing Remarks                                   |

## Accessibility and Modality

All accepted papers and posters will be available online for remote attendees. The talks will be recorded and made available on YouTube. We will also have a dedicated Discord channel for virtual engagement, ensuring that participants, whether in-person or remote, can collaborate and discuss ideas.

## Invited Speakers

We are honored to have several leading experts who will provide keynote presentations. Each speaker brings unique insights into machine learning theory, weight space analysis, and neural network synthesis. Full list will be available at a later date.

<!--- **Chelsea Finn** (Stanford University)
- **Michael Mahoney** (UC Berkeley)
- **Stella Yu** (University of Michigan)
- **Boris Knyazev** (Samsung SAIT AI Lab)
- **Naomi Saphra** (Harvard University)-->

## Organizing Committee

- **Konstantin Schürholt** (University of St. Gallen)
- **Giorgos Bouritsas** (Archimedes AI & University of Athens).
- **Eliahu Horwitz** (HUJI)
- **Derek Lim** (MIT & Liquid AI)
- **Yoav Gelberg** (University of Oxford)
- **Bo Zhao** (University of California, San Diego)
- **Allan Zhou** (Google Deepmind)
- **Damian Borth** (HSG, UW & TU/e)
- **Stefanie Jegelka** (MIT & TU Munich)


## Submission and Important Dates

- **Submission Deadline**: February 3, 2025
- **Author Notification**: March 5, 2025
- **Camera-Ready Deadline**: March 26, 2025
- **Workshop Day**: April 27 or 28, 2025

